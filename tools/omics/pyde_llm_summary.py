import pandas as pd
import os
import glob
import time
import re
import json
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from dotenv import load_dotenv

# === é…ç½® DeepSeek ===
BASE_URL = "https://api.deepseek.com/chat/completions"
load_dotenv()
API_KEY = os.getenv("DEEPSEEK_API_KEY")
MODEL_NAME = "deepseek-chat"

# =========================
# è·¯å¾„ä¸å¹¶å‘é…ç½®
# =========================
INPUT_DIR = "D:/Bit/tools/data/LLM_Input_Ready"
OUTPUT_DIR = "D:/Bit/tools/data/Final_LLM_Results"
MAX_WORKERS = 4  # æµ‹è¯•æ—¶å¹¶å‘è®¾å°ä¸€ç‚¹ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
TEST_LIMIT = 9999  # âš ï¸ ä»…å¤„ç†å‰ 10 ä¸ªåŸºå› 


# =========================
# DeepSeek è°ƒç”¨å‡½æ•°ï¼ˆå•æ–‡ä»¶å†…åµŒï¼‰
# =========================
def call_deepseek(prompt, timeout=60):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "You are an expert Bioinformatician assistant."},
            {"role": "user", "content": prompt}
        ],
        "stream": False
    }

    response = requests.post(
        BASE_URL,
        headers=headers,
        json=payload,
        timeout=timeout
    )

    if response.status_code != 200:
        print(f"\n[DeepSeek API Error] Status: {response.status_code}")
        print(response.text)
        response.raise_for_status()

    result = response.json()
    return result["choices"][0]["message"]["content"]


# =========================
# LLM è°ƒç”¨å°è£…ï¼ˆå¸¦é‡è¯•ï¼‰
# =========================
def call_llm_api(prompt, gene_name, retries=3):
    for i in range(retries):
        try:
            return call_deepseek(prompt)
        except Exception as e:
            if i == retries - 1:
                return f"Error: {str(e)}"
            time.sleep(2)
    return "Error: Timeout"


# =========================
# ä» LLM è¾“å‡ºä¸­æå–åˆ†æ•°
# =========================
def extract_score_robust(text):
    """
    ã€æ ¸å¿ƒå‡çº§ã€‘é²æ£’æ€§æå¼ºçš„åˆ†æ•°æå–å‡½æ•°
    èƒ½è¯†åˆ«: "Score: 9/10", "8.5/10", "Resistance Driver Score: 7", "**Score**: 9"
    """
    if not isinstance(text, str):
        return 0.0

    # ç­–ç•¥ 1 (æœ€å‡†): å¯»æ‰¾ "æ•°å­—/10" çš„æ ¼å¼ (e.g., "8/10", "8.5 / 10")
    # pattern: æ•°å­— + 0ä¸ªæˆ–å¤šä¸ªç©ºæ ¼ + / + 0ä¸ªæˆ–å¤šä¸ªç©ºæ ¼ + 10
    match_fraction = re.search(r"(\d+(?:\.\d+)?)\s*/\s*10", text)
    if match_fraction:
        score = float(match_fraction.group(1))
        # é˜²æ­¢æå–å‡ºå¥‡æ€ªçš„æ•°å­— (æ¯”å¦‚æ—¥æœŸ 2023/10)
        if 0 <= score <= 10:
            return score

    # ç­–ç•¥ 2 (å¤‡é€‰): å¯»æ‰¾ "Score: æ•°å­—" æˆ– "Verdict: æ•°å­—"
    # pattern: Score/Verdict + ä»»æ„éæ•°å­—å­—ç¬¦ + æ•°å­—
    match_keyword = re.search(r"(?:Score|Verdict|Rating)[\D]*?(\d+(?:\.\d+)?)", text, re.IGNORECASE)
    if match_keyword:
        score = float(match_keyword.group(1))
        if 0 <= score <= 10:
            return score
            
    return 0.0


# =========================
# å¤„ç†å•ä¸ª CSV æ–‡ä»¶
# =========================
def process_single_file(file_path):
    filename = os.path.basename(file_path)
    drug_name = filename.split('_')[0]
    save_path = os.path.join(OUTPUT_DIR, f"{drug_name}_Final_Report.csv")
    
    print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {drug_name}")
    
    # è¯»å–è¾“å…¥
    df = pd.read_csv(file_path)
    
    # # æ–­ç‚¹ç»­ä¼ é€»è¾‘
    # if os.path.exists(save_path):
    #     df_existing = pd.read_csv(save_path)
    #     processed_genes = df_existing['merge_key'].tolist()
    #     df_to_process = df[~df['merge_key'].isin(processed_genes)].copy()
    #     if df_to_process.empty:
    #         print(f"   âœ… {drug_name} å·²å…¨éƒ¨å®Œæˆï¼Œè·³è¿‡ã€‚")
    #         return
    #     print(f"   ğŸ”„ æ¢å¤è¿›åº¦ï¼šå‰©ä½™ {len(df_to_process)} ä¸ªåŸºå› ")
    # else:
    #     df_to_process = df.copy()
    # âš ï¸ å…³é”®æ­¥éª¤ï¼šåªå–å‰ 10 ä¸ª
    df_test = df.head(TEST_LIMIT).copy()
    print(f"   ğŸ“Š åŸå§‹æ•°æ®æœ‰ {len(df)} ä¸ªåŸºå› ï¼Œæœ¬æ¬¡ä»…æµ‹è¯•å‰ {len(df_test)} ä¸ª: {df_test['merge_key'].tolist()}")
    # å¤šçº¿ç¨‹å¤„ç†
    results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_gene = {
            executor.submit(call_llm_api, row['LLM_Prompt'], row['merge_key']): row 
            for _, row in df_test.iterrows()
        }
        
        # è¿›åº¦æ¡
        for future in tqdm(as_completed(future_to_gene), total=len(df_test), desc=f"Analyzing {drug_name}"):
            row = future_to_gene[future]
            try:
                llm_response = future.result()
                score = extract_score_robust(llm_response)
                
                res_row = row.to_dict()
                res_row['LLM_Response'] = llm_response
                res_row['AI_Score'] = score
                
                results.append(res_row)
                
                # æ‰“å°ç®€æŠ¥ï¼Œæ–¹ä¾¿æ‚¨å®æ—¶çœ‹æ•ˆæœ
                print(f"   âœ… {row['merge_key']}: AIè¯„åˆ† {score}/10")
                
            except Exception as e:
                print(f"âŒ {row['merge_key']} å¤±è´¥: {e}")

    # ä¿å­˜ç»“æœ
    res_df = pd.DataFrame(results)
    res_df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"   ğŸ‰ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {save_path}\n")

def run_batch_llm_analysis():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    input_files = glob.glob(os.path.join(INPUT_DIR, "*_LLM_Input_Deep.csv")) # è¯»å–Deepç‰ˆè¾“å…¥
    
    if not input_files:
        print("âŒ æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ Step 5 æ˜¯å¦è¿è¡ŒæˆåŠŸã€‚")
        return

    print(f"ğŸš€ å¼€å§‹ LLM å°è§„æ¨¡æµ‹è¯• (Top {TEST_LIMIT})...\n")
    
    for file_path in input_files:
        process_single_file(file_path)
        
    print(f"\nğŸ† æµ‹è¯•å®Œæˆï¼è¯·å» {OUTPUT_DIR} æŸ¥çœ‹ CSV æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    run_batch_llm_analysis()
