import time
import logging
import sys
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# === âš™ï¸ é…ç½®åŒºåŸŸ ===
MONGO_HOST = "localhost"
MONGO_PORT = 27017
DB_NAME = "bio"

# 1. è¾“å…¥ï¼šåŸå§‹é‡‡é›†æ•°æ® (åªè¯»)
SOURCE_COLLECTION = "DMLLM"  

# 2. è¾“å‡ºï¼šå¸¦å‘é‡çš„æˆå“æ•°æ® (å†™å…¥)
TARGET_COLLECTION = "DMLLM_EMBEDDING" 

# 3. æ¨¡å‹ï¼šå¿…é¡»ä¸ retriever.py ä¿æŒä¸€è‡´
MODEL_NAME = 'all-MiniLM-L6-v2'        
BATCH_SIZE = 64  # æ ¹æ®æ˜¾å­˜/å†…å­˜è°ƒæ•´ (CPUå»ºè®®32-64, GPUå¯ä»¥128-256)

# === æ—¥å¿—é…ç½® ===
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def run_vectorization():
    # --- 1. è¿æ¥æ•°æ®åº“ ---
    try:
        client = MongoClient(MONGO_HOST, MONGO_PORT)
        db = client[DB_NAME]
        source_col = db[SOURCE_COLLECTION]
        target_col = db[TARGET_COLLECTION]
        
        # ä¸ºç›®æ ‡é›†åˆåˆ›å»ºç´¢å¼• (åŠ é€Ÿæ£€ç´¢å’Œå»é‡)
        target_col.create_index("pmid", unique=True)
        target_col.create_index("source_tag")
        
        logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        logger.info(f"   ğŸ“‚ æºæ•°æ® (Raw): {SOURCE_COLLECTION}")
        logger.info(f"   ğŸ“‚ ç›®æ ‡åº“ (Vec): {TARGET_COLLECTION}")
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return

    # --- 2. æ£€æŸ¥æ–­ç‚¹ (Smart Resume) ---
    logger.info("ğŸ” æ­£åœ¨æ£€æŸ¥å¢é‡çŠ¶æ€...")
    # è·å–ç›®æ ‡åº“é‡Œå·²å­˜åœ¨çš„ PMIDï¼Œæ”¾å…¥å†…å­˜ Set ä¸­
    existing_cursor = target_col.find({}, {"pmid": 1})
    existing_pmids = set(doc['pmid'] for doc in existing_cursor if 'pmid' in doc)
    logger.info(f"   ğŸ“Š ç›®æ ‡åº“å·²åŒ…å« {len(existing_pmids)} æ¡æ•°æ® (å°†è‡ªåŠ¨è·³è¿‡)ã€‚")

    # --- 3. ç»Ÿè®¡ä»»åŠ¡é‡ ---
    total_source = source_col.count_documents({})
    logger.info(f"   ğŸ“Š æºæ•°æ®å…±æœ‰ {total_source} æ¡ã€‚")
    
    if len(existing_pmids) >= total_source:
        logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®å‡å·²å‘é‡åŒ–ï¼Œæ— éœ€æ“ä½œï¼")
        return

    # --- 4. åŠ è½½æ¨¡å‹ ---
    logger.info(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME}...")
    try:
        model = SentenceTransformer(MODEL_NAME)
        device = model.device
        logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è¿è¡Œè®¾å¤‡: {device})")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # --- 5. æ‰¹é‡å¤„ç†ä¸»å¾ªç¯ ---
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œåˆå§‹ä½ç½®è®¾ä¸ºå·²å®Œæˆçš„æ•°é‡
    pbar = tqdm(total=total_source, initial=len(existing_pmids), desc="Vectorizing", unit="docs")
    
    # æ¸¸æ ‡éå†æºæ•°æ®
    source_cursor = source_col.find({}, batch_size=500)
    
    batch_docs = []
    
    for doc in source_cursor:
        pmid = doc.get('pmid')
        
        # [å…³é”®] å¦‚æœç›®æ ‡åº“æœ‰äº†ï¼Œç›´æ¥è·³è¿‡ (æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒ)
        if pmid in existing_pmids:
            continue
            
        batch_docs.append(doc)
        
        # æ”’å¤Ÿä¸€ä¸ª Batch å°±å¤„ç†
        if len(batch_docs) >= BATCH_SIZE:
            _process_and_insert_batch(batch_docs, model, target_col)
            pbar.update(len(batch_docs))
            batch_docs = [] # æ¸…ç©ºç¼“å­˜

    # å¤„ç†å‰©ä½™çš„å°¾å·´
    if batch_docs:
        _process_and_insert_batch(batch_docs, model, target_col)
        pbar.update(len(batch_docs))

    pbar.close()
    logger.info("âœ… å‘é‡åŒ–ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼Agent ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™äº›æ•°æ®äº†ã€‚")

def _process_and_insert_batch(docs, model, target_col):
    """
    å¤„ç†é€»è¾‘ï¼š
    1. æå–æ–‡æœ¬
    2. è®¡ç®—å‘é‡
    3. å¤åˆ¶åŸå§‹å…ƒæ•°æ® + æ’å…¥å‘é‡å­—æ®µ
    4. å†™å…¥æ–°è¡¨
    """
    if not docs: return

    texts = []
    valid_docs = []
    
    # 1. æå–æœ‰æ•ˆæ–‡æœ¬
    for d in docs:
        text_content = d.get('text', '')
        # ç®€å•æ¸…æ´—ï¼šå»é™¤å¤ªçŸ­çš„æ— æ•ˆæ–‡æœ¬
        if text_content and len(text_content.strip()) > 5:
            texts.append(text_content)
            valid_docs.append(d)
    
    if not texts: return

    try:
        # 2. è®¡ç®—å‘é‡ (Embedding)
        # show_progress_bar=False é¿å…å’Œå¤–å±‚ tqdm å†²çª
        embeddings = model.encode(texts, batch_size=len(texts), show_progress_bar=False)
        
        # 3. ç»„è£…æ–°æ–‡æ¡£
        docs_to_insert = []
        for original_doc, vec in zip(valid_docs, embeddings):
            # [æ ¸å¿ƒæ­¥éª¤] å¤åˆ¶åŸå§‹å¯¹è±¡ï¼Œç¡®ä¿ä¿ç•™ PMID, Title, Journal ç­‰ä¿¡æ¯
            new_doc = original_doc.copy()
            
            # åˆ é™¤åŸæœ‰çš„ _idï¼Œè®© MongoDB åœ¨æ–°é›†åˆé‡Œç”Ÿæˆæ–°çš„ï¼Œé¿å…ä¸»é”®å†²çª
            if '_id' in new_doc:
                del new_doc['_id']
            
            # æ³¨å…¥å‘é‡å­—æ®µ
            new_doc['vector'] = vec.tolist()
            new_doc['vectorized_at'] = time.time()
            
            docs_to_insert.append(new_doc)
        
        # 4. æ‰¹é‡å†™å…¥
        if docs_to_insert:
            # ordered=False å…è®¸éƒ¨åˆ†æˆåŠŸ (å³ä½¿æŸæ¡å› æ„å¤–é‡å¤æŠ¥é”™ï¼Œå…¶ä»–ä¹Ÿèƒ½æ’è¿›å»)
            target_col.insert_many(docs_to_insert, ordered=False)
            
    except Exception as e:
        # å¿½ç•¥ Duplicate Key Error (E11000)ï¼Œåªæ‰“å°å…¶ä»–é”™è¯¯
        if "E11000" not in str(e):
            logger.error(f"âš ï¸ Batch Insert Error: {e}")

if __name__ == "__main__":
    run_vectorization()