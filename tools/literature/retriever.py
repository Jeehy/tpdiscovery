import logging
import re
import time
import numpy as np
import faiss
from typing import List, Dict, Tuple
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class LiteratureRetriever:
    """
    æ–‡çŒ®æ£€ç´¢å·¥å…· (çº¯å‡€æœ¬åœ°ç‰ˆ)
    
    èŒè´£ï¼š
    1. ç®¡ç†æœ¬åœ° MongoDB (DMLLM_EMBEDDING) å’Œ FAISS ç´¢å¼•èµ„æº
    2. ç”Ÿæˆ Discovery/Validation æ¨¡å¼çš„æŸ¥è¯¢ç­–ç•¥
    3. æ‰§è¡Œ Hybrid Search (Vector + Keyword + Year Weighting)
    4. è¿”å›å¸¦æœ‰å®Œæ•´æº¯æºä¿¡æ¯çš„æ–‡çŒ®åˆ—è¡¨
    """
    
    def __init__(self, host: str = "localhost", 
                 port: int = 27017, 
                 db_name: str = "bio", 
                 collection_name: str = "DMLLM_EMBEDDING"):
        self.host = host
        self.port = port
        self.db_name = db_name
        self.collection_name = collection_name
        
        # èµ„æºå ä½
        self.model = None
        self.index = None
        self.doc_ids = []
        self.client = None
        self.collection = None

    def _connect_db(self):
        """è¿æ¥æ•°æ®åº“ (Lazy Load)"""
        if self.client: return
        try:
            self.client = MongoClient(host=self.host, port=self.port, serverSelectionTimeoutMS=2000)
            self.collection = self.client[self.db_name][self.collection_name]
            logger.info(f"âœ… [Retriever] å·²è¿æ¥æ•°æ®åº“: {self.db_name}.{self.collection_name}")
        except Exception as e:
            logger.error(f"DB Connection Error: {e}")

    def _ensure_resources(self):
        """åŠ è½½æ¨¡å‹ä¸æ„å»ºç´¢å¼•"""
        if self.model and self.index: return
        
        self._connect_db()
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            if not self.model:
                logger.info("â³ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # 2. æ„å»ºç´¢å¼• (åªåŠ è½½å‘é‡å’ŒIDï¼Œä¸åŠ è½½æ–‡æœ¬ä»¥èŠ‚çœå†…å­˜)
            if self.collection is not None and self.index is None:
                logger.info("â³ æ­£åœ¨æ„å»ºæœ¬åœ° FAISS ç´¢å¼• (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
                start_time = time.time()
                
                # åªæŸ¥ vector å’Œ _id
                cursor = self.collection.find(
                    {"vector": {"$exists": True}}, 
                    {"vector": 1, "_id": 1} 
                )
                
                vectors = []
                self.doc_ids = [] # ç”¨äºä» FAISS index æ˜ å°„å› MongoDB _id
                
                for doc in cursor:
                    vec = doc.get('vector')
                    if vec and len(vec) == 384: # ç¡®ä¿ç»´åº¦æ­£ç¡®
                        vectors.append(vec)
                        self.doc_ids.append(doc['_id'])
                
                if vectors:
                    # è½¬ä¸º float32 çŸ©é˜µ
                    vectors_np = np.array(vectors).astype('float32')
                    
                    # å½’ä¸€åŒ– (è®©å†…ç§¯ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦)
                    faiss.normalize_L2(vectors_np)
                    
                    # åˆ›å»º FAISS ç´¢å¼• (Inner Product)
                    dimension = vectors_np.shape[1]
                    self.index = faiss.IndexFlatIP(dimension)
                    self.index.add(vectors_np)
                    
                    elapsed = time.time() - start_time
                    logger.info(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼åŒ…å« {self.index.ntotal} æ¡å‘é‡ï¼Œè€—æ—¶ {elapsed:.2f}s")
                else:
                    logger.warning("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å‘é‡æ•°æ®ï¼")
                    
        except Exception as e:
            logger.warning(f"Local Resources Load Failed: {e}")

    def _calculate_keyword_score(self, query: str, text: str) -> float:
        """å…³é”®è¯é‡åˆåº¦æ‰“åˆ†"""
        if not query or not text: return 0.0
        q_terms = set(re.findall(r'\w+', query.lower()))
        t_terms = set(re.findall(r'\w+', text.lower()))
        if not q_terms: return 0.0
        return len(q_terms.intersection(t_terms)) / len(q_terms)

    # === æ ¸å¿ƒæ£€ç´¢é€»è¾‘ (FAISS + MongoDBå›æŸ¥ + æ··åˆæ‰“åˆ†) ===
    def _search_local(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æ‰§è¡Œæ­¥éª¤ï¼š
        1. Query -> Vector
        2. FAISS æ£€ç´¢ -> Top K IDs
        3. MongoDB æ‰¹é‡æŸ¥è¯¦æƒ… (Text, Metadata)
        4. æ··åˆæ‰“åˆ† (Vector + Keyword + Year)
        """
        if not self.index or not self.model: return []
        
        try:
            # 1. å‘é‡ç¼–ç 
            q_vec = self.model.encode([query])
            q_vec = np.array(q_vec).astype('float32')
            faiss.normalize_L2(q_vec) # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
            
            # 2. FAISS æœç´¢ (å¤šå–ä¸€ç‚¹åšé‡æ’)
            D, I = self.index.search(q_vec, top_k * 2)
            
            # è·å–å‘½ä¸­çš„ MongoDB ID å’Œ å‘é‡åˆ†æ•°
            hit_ids = []
            vec_scores = {}
            
            for rank, idx in enumerate(I[0]):
                if idx == -1: continue
                mongo_id = self.doc_ids[idx]
                hit_ids.append(mongo_id)
                vec_scores[mongo_id] = float(D[0][rank]) # è®°å½• FAISS åˆ†æ•°
            
            if not hit_ids: return []

            # 3. å›æŸ¥ MongoDB è·å–æ–‡æœ¬è¯¦æƒ…
            cursor = self.collection.find(
                {"_id": {"$in": hit_ids}},
                {"text": 1, "paper_title": 1, "metadata": 1, "pmid": 1}
            )
            
            results = []
            for doc in cursor:
                doc_id = doc['_id']
                text = doc.get('text', '')
                title = doc.get('paper_title', 'Unknown')
                
                # --- æ··åˆæ‰“åˆ†é€»è¾‘ ---
                vec_score = vec_scores.get(doc_id, 0.0)
                kw_score = self._calculate_keyword_score(query, text)
                
                # æƒé‡: å‘é‡ 70%, å…³é”®è¯ 30%
                hybrid_score = (0.7 * vec_score) + (0.3 * kw_score)
                
                # ç« èŠ‚/å¹´ä»½åŠ æƒ
                multiplier = 1.0
                metadata = doc.get('metadata', {})
                year = str(metadata.get('year', ''))
                
                # ç»™æœ€è¿‘ 3-4 å¹´çš„æ–‡çŒ®åŠ åˆ†
                if year in ['2023', '2024', '2025', '2026']:
                    multiplier = 1.1
                
                final_score = round(hybrid_score * multiplier, 4)
                
                # ç»„è£…è¯æ®å¯¹è±¡ (å¸¦åŸæ–‡é“¾æ¥)
                pmid = doc.get('pmid', 'N/A')
                url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid != "N/A" else ""
                
                author = metadata.get('author', 'Unknown')
                journal = metadata.get('journal', 'Journal')
                citation = f"{author} et al., {journal} ({year})"

                results.append({
                    "content": text,
                    "score": final_score,
                    "source": "Local_DB",
                    "search_aspect": "unknown", # åœ¨ä¸Šå±‚èµ‹å€¼
                    
                    # æä¾›å®Œæ•´çš„å…ƒæ•°æ®ç»™ Agent
                    "source_metadata": {
                        "title": title,
                        "pmid": pmid,
                        "url": url,
                        "citation": citation,
                        "year": year
                    },
                    # Markdown å¼•ç”¨æ ¼å¼
                    "reference": f"[{citation}]({url}) - {title}"
                })
            
            # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]

        except Exception as e:
            logger.error(f"Local search error: {e}")
            return []

    def _generate_queries(self, gene: str, disease: str, mode: str) -> List[Tuple[str, str]]:
        queries = []
        
        if mode == "discovery":
            # === æ¢ç´¢æ¨¡å¼ (Discovery) ===
            # ç­–ç•¥ï¼šä¸æœè‚ç™Œï¼Œåªæœæ³›ç™Œç§ã€æœºåˆ¶ã€è¯ç‰©
            queries.append(("pan_cancer", f"{gene} AND (Cancer OR Tumor) AND Review"))
            queries.append(("drug_target", f"{gene} AND (Inhibitor OR Resistance)"))
            queries.append(("mechanism", f"{gene} AND (Signaling Pathways OR Mechanism)"))
        else:
            # === éªŒè¯æ¨¡å¼ (Validation) ===
            # ç­–ç•¥ï¼šå¼ºåˆ¶ç»‘å®šè‚ç™Œå…³é”®è¯
            # 1. ç›´æ¥å…³è”
            queries.append(("direct_link", f"{gene} AND {disease}"))
            # 2. ä¸´åºŠé¢„å
            queries.append(("clinical", f"{gene} AND Prognosis AND {disease}"))
            # 3. ç‰¹å®šè€è¯/æœºåˆ¶ (Specific Mechanism) - å·²è¡¥å›
            queries.append(("mechanism", f"{gene} AND ({disease} OR HCC) AND (Resistance OR Metastasis)"))
            
        return queries

    def get_evidence(self, gene: str, disease: str = "liver cancer", mode: str = "discovery") -> List[Dict]:
        """
        è·å–è¯æ®çš„ä¸»å…¥å£
        """
        self._ensure_resources()
        queries = self._generate_queries(gene, disease, mode)
        
        all_results = []
        seen_pmids = set()
        
        for aspect, q_str in queries:
            logger.info(f"ğŸ” [Local Search] Aspect [{aspect}]: {q_str}")
            
            # æœ¬åœ°æ£€ç´¢ Top 5
            results = self._search_local(q_str, top_k=5)
            
            for res in results:
                pmid = res['source_metadata'].get('pmid') 
                if pmid and pmid not in seen_pmids:
                    res['search_aspect'] = aspect # æ ‡è®°æ˜¯å“ªç§è§’åº¦æœå‡ºæ¥çš„
                    all_results.append(res)
                    seen_pmids.add(pmid)
        
        # å†æ¬¡æŒ‰åˆ†æ•°æ’åº
        all_results.sort(key=lambda x: x['score'], reverse=True)
        
        logger.info(f"âœ… å…±æ£€ç´¢åˆ° {len(all_results)} æ¡æœ¬åœ°è¯æ®")
        return all_results
    
# === è‡ªæµ‹ä»£ç  ===
if __name__ == "__main__":
    retriever = LiteratureRetriever()
    
    # æµ‹è¯• Validation æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦ä¼šæ‰§è¡Œ 3 ä¸ªç­–ç•¥ (å« specific mechanism)
    test_gene = "TP53"
    print(f"\nğŸš€ æµ‹è¯•æ£€ç´¢: {test_gene} (Validation Mode)...")
    
    results = retriever.get_evidence(test_gene, mode="validation")
    
    print(f"\nâœ… æ‰¾åˆ° {len(results)} æ¡è¯æ®ï¼š")
    for i, res in enumerate(results[:3]):
        print(f"\n--- [Result {i+1}] (Aspect: {res.get('search_aspect')}) ---")
        print(f"ğŸ“„ {res['reference']}")
        print(f"ğŸ“ {res['content'][:100]}...")