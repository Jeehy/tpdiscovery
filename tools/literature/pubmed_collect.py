import time
import logging
import re
import sys
import urllib.error
from Bio import Entrez
from pymongo import MongoClient
from tqdm import tqdm

# === ğŸ› ï¸ å½»åº•ä¿®å¤ Windows ç¼–ç å´©æºƒé—®é¢˜ ===
if sys.platform.startswith('win'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception as e:
        pass

# === âš™ï¸ é…ç½®åŒºåŸŸ ===
MONGO_HOST = "localhost"
MONGO_PORT = 27017
DB_NAME = "bio"
COLLECTION_NAME = "DMLLM"

Entrez.email = "826329938@qq.com" 
ENTREZ_API_KEY = None  

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("dmllm_collection.log", encoding='utf-8'), 
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MassCollector:
    def __init__(self):
        try:
            self.client = MongoClient(MONGO_HOST, MONGO_PORT)
            self.db = self.client[DB_NAME]
            self.collection = self.db[COLLECTION_NAME]
            
            # åˆ›å»ºç´¢å¼•
            self.collection.create_index("pmid", unique=True)
            self.collection.create_index("source_tag")
            self.collection.create_index("processed")
            
            logger.info(f"âœ… å·²è¿æ¥ MongoDB: {DB_NAME}.{COLLECTION_NAME}")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def run_strategy(self, search_queries: dict):
        total_strategies = len(search_queries)
        # æŒ‰å¹´ä»½æ’åºæ‰§è¡Œï¼Œä½“éªŒæ›´å¥½
        sorted_keys = sorted(search_queries.keys(), reverse=True)
        
        for idx, name in enumerate(sorted_keys, 1):
            query = search_queries[name]
            logger.info(f"\nğŸš€ [ä»»åŠ¡ {idx}/{total_strategies}] å¯åŠ¨: {name}")
            # logger.info(f"   Query: {query[:100]}...") # æ—¥å¿—å¤ªé•¿å¯å–æ¶ˆæ³¨é‡Š
            self._download_by_query(query, tag=name)

    def _get_search_session(self, query):
        """æ‰§è¡Œæœç´¢å¹¶è¿”å› Session ä¿¡æ¯"""
        try:
            handle = Entrez.esearch(
                db="pubmed", term=query, usehistory="y", api_key=ENTREZ_API_KEY
            )
            res = Entrez.read(handle)
            handle.close()
            return {
                "count": int(res["Count"]),
                "webenv": res["WebEnv"],
                "query_key": res["QueryKey"]
            }
        except Exception as e:
            logger.error(f"   âš ï¸ Search Session è·å–å¤±è´¥: {e}")
            return None

    def _download_by_query(self, query, tag, batch_size=200):
        # 1. åˆå§‹æœç´¢
        session = self._get_search_session(query)
        if not session: return

        count = session['count']
        webenv = session['webenv']
        query_key = session['query_key']

        if count == 0:
            logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°æ–‡çŒ® (Count=0)ï¼Œè·³è¿‡ã€‚")
            return

        # 2. æ–­ç‚¹ç»­ä¼ è®¡ç®—
        existing_count = self.collection.count_documents({"source_tag": tag})
        start_index = 0
        if existing_count > 0:
            # å›é€€ä¸€ä¸ª batch é˜²æ­¢æ¼æ•°æ®
            start_index = max(0, (existing_count // batch_size) * batch_size - batch_size)
            logger.info(f"   ğŸ”„ [æ–­ç‚¹ç»­ä¼ ] åº“ä¸­å·²æœ‰ {existing_count} æ¡ï¼Œä»ç´¢å¼• {start_index}/{count} ç»§ç»­...")
        else:
            logger.info(f"   ğŸ” å‘½ä¸­ {count} ç¯‡ï¼Œå‡†å¤‡ä¸‹è½½...")

        # 3. åˆ†æ‰¹ä¸‹è½½å¾ªç¯
        pbar = tqdm(total=count, initial=start_index, desc=f"ğŸ“¥ {tag}")
        
        current_start = start_index
        while current_start < count:
            success = self._fetch_and_save_with_refresh(
                current_start, batch_size, webenv, query_key, tag, query
            )
            
            if success:
                updated_count = min(batch_size, count - current_start)
                pbar.update(updated_count)
                current_start += batch_size
                time.sleep(0.3 if ENTREZ_API_KEY else 0.5)
            else:
                logger.warning(f"   ğŸ”„ Batch {current_start} å¤±è´¥ï¼Œåˆ·æ–° Session é‡è¯•...")
                new_session = self._get_search_session(query)
                if new_session:
                    webenv = new_session['webenv']
                    query_key = new_session['query_key']
                    time.sleep(2)
                else:
                    logger.error("   âŒ Session åˆ·æ–°å¤±è´¥ï¼Œåœæ­¢å½“å‰ç­–ç•¥ã€‚")
                    break
        pbar.close()

    def _fetch_and_save_with_refresh(self, start, batch_size, webenv, query_key, tag, query):
        max_retries = 5
        for attempt in range(max_retries):
            try:
                handle = Entrez.efetch(
                    db="pubmed", 
                    retstart=start, 
                    retmax=batch_size,
                    webenv=webenv, 
                    query_key=query_key,
                    rettype="medline", 
                    retmode="text", 
                    api_key=ENTREZ_API_KEY
                )
                data = handle.read()
                handle.close()
                
                if not data: return True 

                self._parse_and_insert(data, tag)
                return True

            except urllib.error.HTTPError as e:
                if e.code == 400:
                    logger.warning(f"   âš ï¸ HTTP 400 (Bad Request). Session å¤±æ•ˆã€‚")
                    return False # è¿”å› False è¯·æ±‚å¤–å±‚åˆ·æ–°
                time.sleep(3 * (attempt + 1))
            except Exception as e:
                time.sleep(3 * (attempt + 1))
        
        return False

    def _parse_and_insert(self, raw_text, tag):
        records = raw_text.split("\n\n")
        docs = []
        for rec in records:
            if not rec.strip(): continue
            
            pmid_match = re.search(r"PMID- (\d+)", rec)
            if not pmid_match: continue
            pmid = pmid_match.group(1)
            
            if self.collection.count_documents({"pmid": pmid}, limit=1):
                continue

            ab_match = re.search(r"AB\s+-\s+(.*?)\n[A-Z]", rec, re.DOTALL)
            abstract = ab_match.group(1).replace("\n      ", " ") if ab_match else ""
            if len(abstract) < 50: continue 

            ti_match = re.search(r"TI\s+-\s+(.*?)\n[A-Z]", rec, re.DOTALL)
            title = ti_match.group(1).replace("\n      ", " ") if ti_match else "Unknown Title"

            year = self._extract_regex(r"DP\s+-\s+(\d{4})", rec)
            journal = self._extract_regex(r"TA\s+-\s+(.*?)\n", rec)
            author = self._extract_regex(r"AU\s+-\s+(.*?)\n", rec)

            doc = {
                "pmid": pmid,
                "paper_title": title,
                "text": abstract,
                "section": "Abstract",
                "source_tag": tag,
                "source_filename": "Local_DMLLM",
                "processed": False,
                "metadata": {
                    "year": year,
                    "journal": journal,
                    "author": author,
                    "citation": f"{author} et al., {year}, {journal}"
                },
                "crawled_at": time.time()
            }
            docs.append(doc)

        if docs:
            try:
                self.collection.insert_many(docs, ordered=False)
            except Exception:
                pass 

    def _extract_regex(self, pattern, text):
        m = re.search(pattern, text, re.DOTALL)
        return m.group(1).replace("\n      ", " ") if m else "Unknown"

# ==========================================
# ğŸ¯ ä¸»ç¨‹åºï¼šå…¨ç­–ç•¥ç”Ÿæˆé€»è¾‘
# ==========================================
if __name__ == "__main__":
    collector = MassCollector()
    
    # 1. å®šä¹‰ä½ çš„æ ¸å¿ƒæ£€ç´¢é€»è¾‘ (Base Queries)
    # è¿™äº›æ˜¯ä½ è¦æ‰¾çš„æ‰€æœ‰æ–¹å‘ï¼Œæˆ‘ä»¬ä¿æŒå®ƒä»¬çš„å®Œæ•´æ€§
    BASE_QUERIES = {
        # [Validation] è‚ç™Œæ·±åº¦éªŒè¯
        "Validation_Liver": '("Carcinoma, Hepatocellular"[MeSH] OR "Liver Neoplasms"[MeSH]) AND (Review[pt] OR Clinical Trial[pt])',
        
        # [Discovery] æ³›ç™Œç§æœºåˆ¶ (Review)
        "Discovery_PanCancer": '(Neoplasms[MeSH] AND (Signaling Pathways OR Molecular Mechanisms)) AND Review[pt]',
        
        # [Discovery] è€è¯æ€§ä¸é¶å‘ (é¡¶åˆŠ)
        "Discovery_Drug_Resistance": '("Drug Resistance, Neoplasm"[MeSH] OR "Molecular Targeted Therapy") AND (Inhibitor OR Antagonist) AND ("Nature"[Journal] OR "Cell"[Journal] OR "Science"[Journal] OR "Cancer Cell"[Journal] OR "Hepatology"[Journal])',
        
        # [Discovery] æ–°å…´çƒ­ç‚¹ (é“æ­»äº¡ç­‰)
        "Discovery_Emerging_Topics": '(Ferroptosis OR Pyroptosis OR "Immune Checkpoint" OR "Metabolic Reprogramming" OR "Liquid Biopsy") AND Neoplasms'
    }

    FINAL_STRATEGIES = {}
    
    # 2. è‡ªåŠ¨åˆ†å¹´å¤„ç† (2015 - 2026)
    # ä¸ºä»€ä¹ˆæ‰€æœ‰ç­–ç•¥éƒ½è¦åˆ†å¹´ï¼Ÿå› ä¸º "è€è¯æ€§" æˆ– "æ³›ç™Œ" çš„æ€»æ•°ä¹Ÿæå¯èƒ½è¶…è¿‡ 1ä¸‡æ¡ã€‚
    # åˆ†å¹´æ˜¯é¿å… HTTP 400 é”™è¯¯æœ€ä¿é™©çš„æ–¹æ³•ã€‚
    START_YEAR = 2015
    END_YEAR = 2026 
    SPLIT_YEAR_THRESHOLD = 2020 # ä»2020å¹´å¼€å§‹æ‹†åˆ†ä¸ŠåŠå¹´å’Œä¸‹åŠå¹´
    
    print("ğŸ“‹ æ­£åœ¨ç”Ÿæˆç­–ç•¥ (2020å¹´åè‡ªåŠ¨å¯ç”¨åŠå¹´åˆ‡åˆ†æ¨¡å¼)...")
    
    print("ğŸ“‹ æ­£åœ¨ç”Ÿæˆå…¨ç»´åº¦åˆ†å¹´æ£€ç´¢ç­–ç•¥...")
    
    for base_name, base_query in BASE_QUERIES.items():
        for year in range(START_YEAR, END_YEAR + 1):
            
            if year < SPLIT_YEAR_THRESHOLD:
                # === æ¨¡å¼ A: æ•´å¹´ (é€‚åˆè€æ•°æ®) ===
                key = f"{base_name}_{year}"
                time_filter = f' AND "{year}/01/01"[Date - Publication] : "{year}/12/31"[Date - Publication]'
                FINAL_STRATEGIES[key] = base_query + time_filter
                
            else:
                # === æ¨¡å¼ B: åŠå¹´æ‹†åˆ† (é€‚åˆæ–°æ•°æ®ï¼Œé¿å¼€ 10k é™åˆ¶) ===
                # ä¸ŠåŠå¹´ (Part A)
                key_a = f"{base_name}_{year}_PartA" # Jan - Jun
                time_filter_a = f' AND "{year}/01/01"[Date - Publication] : "{year}/06/30"[Date - Publication]'
                FINAL_STRATEGIES[key_a] = base_query + time_filter_a
                
                # ä¸‹åŠå¹´ (Part B)
                key_b = f"{base_name}_{year}_PartB" # Jul - Dec
                time_filter_b = f' AND "{year}/07/01"[Date - Publication] : "{year}/12/31"[Date - Publication]'
                FINAL_STRATEGIES[key_b] = base_query + time_filter_b

    print(f"ğŸš€ ç­–ç•¥ç”Ÿæˆå®Œæ¯•ï¼")
    print(f"   - æ€»ä»»åŠ¡æ•°: {len(FINAL_STRATEGIES)}")
    print(f"   - è¯´æ˜: {SPLIT_YEAR_THRESHOLD}å¹´åŠä»¥åå·²æ‹†åˆ†ä¸º PartA/PartB ä»¥ç¡®ä¿å°äº 10000 æ¡ã€‚")
    print("ğŸ”¥ å¼€å§‹æ‰§è¡Œ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )...")
    
    collector.run_strategy(FINAL_STRATEGIES)
    
    print("\nâœ… æ‰€æœ‰é‡‡é›†ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼")