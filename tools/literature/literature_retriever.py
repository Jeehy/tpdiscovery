import logging
import re
import numpy as np
import faiss
from typing import List, Dict, Tuple
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
from tools.literature.pubmed_tool import PubMedTool  # å¤ç”¨å·²æ”¹å¥½çš„ PubMedTool

logger = logging.getLogger(__name__)

class LiteratureRetriever:
    """
    æ–‡çŒ®æ£€ç´¢å·¥å…· (çº¯å‡€ç‰ˆ)
    
    èŒè´£ï¼š
    1. ç®¡ç†æœ¬åœ° MongoDB å’Œ FAISS ç´¢å¼•èµ„æº
    2. ç”Ÿæˆ Discovery/Validation æ¨¡å¼çš„æŸ¥è¯¢ç­–ç•¥
    3. æ‰§è¡Œ Hybrid Search (Local Vector + Online PubMed)
    4. è¿”å›åŸå§‹æ–‡çŒ®åˆ—è¡¨ (æ—  LLM ä»‹å…¥)
    """
    
    def __init__(self, host: str = "localhost", port: int = 27017, 
                 db_name: str = "bio", collection_name: str = "evidence_chunks"):
        self.host = host
        self.port = port
        self.db_name = db_name
        self.collection_name = collection_name
        
        # èµ„æºå ä½
        self.model = None
        self.index = None
        self.doc_map = []
        self.client = None
        self.collection = None
        
        self.pubmed = PubMedTool()

    def _connect_db(self):
        """è¿æ¥æ•°æ®åº“ (Lazy Load)"""
        if self.client: return
        try:
            self.client = MongoClient(host=self.host, port=self.port, serverSelectionTimeoutMS=2000)
            self.collection = self.client[self.db_name][self.collection_name]
        except Exception as e:
            logger.error(f"DB Connection Error: {e}")

    def _ensure_resources(self):
        """åŠ è½½æ¨¡å‹ä¸æ„å»ºç´¢å¼• (ä¿ç•™åŸé€»è¾‘)"""
        if self.model: return
        self._connect_db()
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # 2. æ„å»ºç´¢å¼•
            if self.collection is not None:
                cursor = self.collection.find(
                    {"vector": {"$exists": True}}, 
                    {"vector": 1, "text": 1, "section": 1, "paper_title": 1, "source_filename": 1}
                )
                vectors = []
                self.doc_map = []
                for doc in cursor:
                    vec = doc.get('vector')
                    if vec:
                        vectors.append(np.array(vec, dtype='float32'))
                        self.doc_map.append({
                            'text': doc.get('text', ''),
                            'section': doc.get('section', 'Unknown'),
                            'title': doc.get('paper_title', 'Unknown'),
                            'source': doc.get('source_filename', 'Local')
                        })
                if vectors:
                    self.index = faiss.IndexFlatIP(vectors[0].shape[0])
                    self.index.add(np.array(vectors))
                    logger.info(f"Local Index built with {len(vectors)} docs.")
        except Exception as e:
            logger.warning(f"Local Resources Load Failed: {e}")

    def _calculate_keyword_score(self, query: str, text: str) -> float:
        """å…³é”®è¯é‡åˆåº¦æ‰“åˆ† (ä¿ç•™åŸé€»è¾‘)"""
        if not query or not text: return 0.0
        q_terms = set(re.findall(r'\w+', query.lower()))
        t_terms = set(re.findall(r'\w+', text.lower()))
        if not q_terms: return 0.0
        return len(q_terms.intersection(t_terms)) / len(q_terms)

    def _search_local(self, query: str, top_k: int = 2) -> List[Dict]:
        if not self.index or not self.model: return []
        try:
            q_vec = self.model.encode([query])
            # å¤šå–ä¸€ç‚¹æ•°æ® (top_k * 2) ç”¨äºé‡æ’
            D, I = self.index.search(np.array(q_vec, dtype='float32'), top_k * 2)
            
            results = []
            for rank, idx in enumerate(I[0]):
                if idx == -1: continue
                doc = self.doc_map[idx]
                
                # --- æ‚¨çš„åŸå§‹æ‰“åˆ†é€»è¾‘ ---
                # 1. åŸå§‹å‘é‡åˆ†
                vec_score = float(D[0][rank])
                # 2. å…³é”®è¯åˆ†
                kw_score = self._calculate_keyword_score(query, doc['text'])
                # 3. æ··åˆæ‰“åˆ†
                hybrid_score = (0.7 * vec_score) + (0.3 * kw_score)
                
                # 4. ç« èŠ‚åŠ æƒ
                section = str(doc.get('section', 'Unknown')).lower()
                multiplier = 1.0
                if any(x in section for x in ['result', 'discussion', 'conclusion']):
                    multiplier = 1.2
                elif 'abstract' in section:
                    multiplier = 1.1
                
                final_score = round(hybrid_score * multiplier, 4)
                # -----------------------

                results.append({
                    "content": doc['text'],
                    # ç»Ÿä¸€ä½¿ç”¨ 'metadata' æ ¼å¼ä»¥é€‚é… Agent
                    "metadata": {
                        "title": doc['title'], 
                        "citation": f"Local: {doc['source']}",
                        "section": doc.get('section', 'Unknown')
                    },
                    "score": final_score, # ä½¿ç”¨åŠ æƒåçš„åˆ†æ•°
                    "source": "Local"
                })
            
            # æŒ‰æœ€ç»ˆåŠ æƒåˆ†æ•°æ’åº
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:top_k]
        except Exception as e:
            logger.error(f"Local search error: {e}")
            return []

    def _generate_queries(self, gene: str, disease: str, mode: str) -> List[Tuple[str, str]]:
        queries = []
        
        if mode == "discovery":
            # === æ¢ç´¢æ¨¡å¼ (Discovery) ===
            # å‡è®¾ï¼šè¯¥åŸºå› åœ¨è‚ç™Œä¸­æ˜¯æœªçŸ¥çš„ã€‚
            # ç­–ç•¥ï¼šä¸æœè‚ç™Œï¼Œåªæœæ³›ç™Œç§ã€æœºåˆ¶ã€è¯ç‰©ã€‚
            
            # 1. æ³›ç™Œç§å…³è” (Pan-Cancer)
            # æ„å›¾ï¼šå¯»æ‰¾å®ƒåœ¨å…¶ä»–ç™Œç—‡ï¼ˆè‚ºç™Œã€ä¹³è…ºç™Œç­‰ï¼‰ä¸­çš„è‡´ç™Œè¯æ®
            queries.append(("pan_cancer", f"{gene}[Title] AND (Cancer OR Tumor OR Carcinoma)"))
            
            # 2. è¯ç‰©é¶ç‚¹æ½œåŠ› (Druggability)
            # æ„å›¾ï¼šå¯»æ‰¾æ˜¯å¦æœ‰ç°æˆçš„æŠ‘åˆ¶å‰‚æˆ–è€è¯æœºåˆ¶
            queries.append(("drug_target", f"{gene}[Title/Abstract] AND (Inhibitor OR Drug OR Resistance)"))
            
            # 3. æ ¸å¿ƒæœºåˆ¶ (Mechanism)
            # æ„å›¾ï¼šå¯»æ‰¾å®ƒå‚ä¸çš„é€šç”¨ä¿¡å·é€šè·¯ (e.g., Wnt, PI3K)
            queries.append(("mechanism", f"{gene} signaling pathway function"))

        else:
            # === éªŒè¯æ¨¡å¼ (Validation) ===
            # å‡è®¾ï¼šè¯¥åŸºå› ä¸è‚ç™Œæœ‰å¼ºå…³è”ï¼Œéœ€è¦ç¡®è®¤ã€‚
            # ç­–ç•¥ï¼šå¼ºåˆ¶ç»‘å®šè‚ç™Œå…³é”®è¯ã€‚
            
            # 1. ç›´æ¥å…³è” (Direct Link)
            queries.append(("direct_link", f"{gene}[Title] AND ({disease} OR Hepatocellular Carcinoma OR HCC)"))
            
            # 2. ä¸´åºŠé¢„å (Clinical)
            queries.append(("clinical", f"{gene} AND ({disease} OR HCC) AND (Prognosis OR Survival OR Patient)"))
            
            # 3. ç‰¹å®šè€è¯/æœºåˆ¶ (Specific Mechanism)
            queries.append(("mechanism", f"{gene} AND ({disease} OR HCC) AND (Resistance OR Metastasis)"))
            
        return queries

    def get_evidence(self, gene: str, disease: str = "liver cancer", mode: str = "discovery") -> List[Dict]:
        self._ensure_resources()
        queries = self._generate_queries(gene, disease, mode)
        # print(f"    ğŸ” [Retriever] {mode.upper()} Search for {gene} ({len(queries)} queries)...")
        
        combined_results = []
        seen_hashes = set()
        
        for aspect, q_str in queries:
            k_online = 3 if mode == "discovery" else 2
            k_local = 2
            
            # 1. åœ¨çº¿æ£€ç´¢
            online_raw = self.pubmed.search(q_str, max_results=k_online)
            
            # 2. æœ¬åœ°æ£€ç´¢ (å¸¦åŠ æƒ)
            local_res = self._search_local(q_str, top_k=k_local)
            
            # 3. æ ¼å¼åŒ–åœ¨çº¿ç»“æœ (æ ‡å‡†åŒ–æ¸…æ´—)
            formatted_online = []
            for item in online_raw:
                raw_meta = item.get('source_metadata', {})
                formatted_online.append({
                    "content": item['content'],
                    "metadata": {
                        "title": raw_meta.get('paper_title', 'Unknown Title'), 
                        "citation": raw_meta.get('citation_str', 'PubMed')
                    },
                    "score": 0.9, 
                    "source": "Online"
                })
            
            # 4. åˆå¹¶å»é‡
            for item in formatted_online + local_res:
                h = hash(item['content'][:100])
                if h not in seen_hashes:
                    item['aspect'] = aspect 
                    combined_results.append(item)
                    seen_hashes.add(h)
        
        # æœ€ç»ˆå†æ¬¡æ’åº (ç¡®ä¿åŠ æƒåçš„æœ¬åœ°ç»“æœèƒ½å’Œåœ¨çº¿ç»“æœæ­£ç¡®ç«äº‰)
        combined_results.sort(key=lambda x: x['score'], reverse=True)
        return combined_results